<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>250 Welcome to Part 9 - Dimensionality Reduction</title>
</head>
<body>
<div class="asset-container">
    <div class="asset-container__padding article-view">
        <div class="w3c-default">
            <p>Welcome to Part 9 - Dimensionality Reduction!</p>

<p><br></p>











<p>Remember in Part 3 - Classification, we worked with datasets composed of only two independent variables. We did for two reasons:</p>







<ol><li>Because we needed two dimensions to visualize better how Machine Learning models worked (by plotting the prediction regions and the prediction boundary for each model).</li><li>Because whatever is the original number of our independent variables, we can often end up with two independent variables by applying an appropriate Dimensionality Reduction technique.</li></ol>



<p><br></p>



<p>There are two types of Dimensionality Reduction techniques:<br></p>







<ol><li>Feature Selection</li><li>Feature Extraction</li></ol>







<p><br></p>



<p>Feature Selection techniques are Backward Elimination, Forward Selection, Bidirectional Elimination, Score Comparison and more. We covered these techniques in Part 2 - Regression.</p>







<p>In this part we will cover the following Feature Extraction techniques:</p>







<ol><li>Principal Component Analysis (PCA)</li><li>Linear Discriminant Analysis (LDA)</li><li>Kernel PCA</li><li>Quadratic Discriminant Analysis (QDA)</li></ol>







<p><br></p>







<p>Enjoy Machine Learning!<br></p>
        </div>
    </div>
</div>

</body>
</html>